<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ava Kouhana - AI research</title>
  <link rel="icon" type="image/png" href="/Users/akouhana/avakn4.github.io/stanford-logo.png">
  <style>
  body {
      font-family: Arial, sans-serif;
      line-height: 1.4;
      color: #333;
      margin: 0;
      padding: 0;
      background-color: #f5f5f5;
    }
    .content {
      max-width: 1200px;
      margin: 0 auto;
      padding: 40px 20px;
    }
    .header {
      display: flex;
      align-items: center;
      margin-bottom: 40px;
    }
    .profile-image {
      width: 270px;
      height: 270px;
      border-radius: 0;
      object-fit: contain;
      margin-right: 40px;
    }
    h1, h2, h3 {
      color: #2c3e50;
      margin-bottom: 10px;
    }
    .contact {
      font-style: italic;
      color: #7f8c8d;
    }
    .profile-links {
      display: flex;
      gap: 10px;
      margin-top: 20px;
    }
    .profile-links a {
      display: inline-flex;
      align-items: center;
      text-decoration: none;
      color: #3498db;
      transition: color 0.3s;
    }
    .profile-links a:hover {
      color: #2980b9;
    }
    .profile-links svg {
      width: 24px;
      height: 24px;
      margin-right: 8px;
    }
    .bio {
      margin-bottom: 40px;
      max-width: 800px;
    }
    .publications, .proceedings {
      margin-top: 40px;
    }
    .paper {
      margin-bottom: 30px;
      padding-bottom: 30px;
      border-bottom: 1px solid #ecf0f1;
    }
    .paper:last-child {
      border-bottom: none;
    }
    .paper-content {
      display: flex;
      align-items: flex-start;
    }
    .paper-image {
      width: 300px;
      height: 187px;
      object-fit: cover;
      margin-right: 20px;
      border-radius: 4px;
    }
    .paper-info {
      flex: 1;
    }
    .paper-title {
      margin: 0 0 5px 0;
      font-size: 18px;
      font-weight: bold;
    }
    .co-authors {
      margin: 0 0 5px 0;
      font-style: italic;
      color: #7f8c8d;
      font-size: 14px;
    }
    .conference-info {
      margin: 0 0 10px 0;
      font-weight: bold;
      color: #e74c3c;
    }
    .paper-links {
      margin-top: 10px;
    }
    .paper-link {
      display: inline-block;
      margin-right: 10px;
      padding: 5px 10px;
      background-color: #3498db;
      color: white;
      text-decoration: none;
      border-radius: 3px;
      font-size: 14px;
    }
    .paper-link:hover {
      background-color: #2980b9;
    }
    .awards {
      margin-top: 40px;
    }
    .awards ul {
      padding-left: 20px;
    }
    .awards li {
      margin-bottom: 10px;
    }
    @media (max-width: 768px) {
      .paper-content {
        flex-direction: column;
      }
      .paper-image {
        width: 100%;
        height: auto;
        margin-right: 0;
        margin-bottom: 20px;
      }
      .header {
        flex-direction: column;
        text-align: center;
      }
      .profile-image {
        margin-right: 0;
        margin-bottom: 20px;
      }
      .profile-links {
        justify-content: center;
      }
    }
  </style>
</head>
<body>
  <div class="content">
    <div class="header">
      <div class="left-section">
        <img src="yournew-photo.jpeg" alt="Ava Kouhana" class="profile-image">
        <div class="contact-info">
          <p>akouhana@stanford.edu</p>
          <div class="profile-links">
            <a href="https://profiles.stanford.edu/ava-kouhana">
              <img src="stanford-logo.png" alt="Stanford logo" width="24" height="24">
              Stanford Profile
            </a>
            <a href="https://scholar.google.com/citations?user=nxESuB0AAAAJ&hl=fr">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
                <path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 
1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z" fill="#4285F4"/>
                <path d="M12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14zm0 11c-2.209 0-4-1.791-4-4s1.791-4 4-4 4 1.791 4 4-1.791 4-4 4z" 
fill="#4285F4"/>
              </svg>
              Google Scholar
            </a>
            <a href="https://www.linkedin.com/in/ava-k-a56a90186/">
              <img src="linkedin-logo.png" alt="LinkedIn logo" width="24" height="24">
              LinkedIn
            </a>
          </div>
        </div>
      </div>
      <div class="info-section">
        <h1>Ava Kouhana</h1>
        <h2>MS Student @Stanford ICME (Computational and Mathematical Engineering)</h2>
        <div class="bio">

<p>I am an incoming ICME master's degree student at Stanford University. Prior to Stanford, I spent six 
months 
conducting research at Harvard Ophthalmology AI lab under the supervision of Dr. Mengyu Wang, focusing 
primarily on Computer Vision tasks like Image Segmentation and Vision-Language Models. Before joining ICME , 
I have had the opportunity to work for six months supervised by Stanford Professor Craig Levin, researching 
the application of Diffusion Models for image super-resolution.<br> 

My research interests primarily revolve around computer vision, deep learning, and generative AI, with a 
growing interest for 3D modeling and video generation. </p>
        </div>
      </div>
    </div>
    <div class="publications">
      <h2>Publications</h2>
      <div class="paper">
        <div class="paper-content">
          <img src="images/fairseg-thumbnail.jpg" alt="FairSeg paper thumbnail" class="paper-image">
          <div class="paper-info">
            <h3 class="paper-title">FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using 
Segment Anything Model with Fair Error-Bound Scaling</h3>
            <p class="co-authors">Yu Tian*, Min Shi*, Yan Luo*, <strong>Ava Kouhana</strong>, Tobias Elze, Mengyu Wang</p>
            <p>ICLR, 2024</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2311.02189" class="paper-link" target="_blank">PDF</a>
              <a href="https://github.com/Harvard-Ophthalmology-AI-Lab/FairSeg" class="paper-link" target="_blank">Code</a>
              <a href="https://drive.google.com/drive/folders/1tyhEhYHR88gFkVzLkJI4gE1BoOHoHdWZ?usp=sharing" 
class="paper-link" target="_blank">Dataset</a>
            </div>
          </div>
        </div>
      </div>
      <div class="paper">
        <div class="paper-content">
          <img src="images/fairclip-thumbnail.jpg" alt="FairCLIP paper thumbnail" class="paper-image">
          <div class="paper-info">
            <h3 class="paper-title">FairCLIP: Harnessing Fairness in Vision-Language Learning</h3>
            <p class="co-authors">Yan Luo*, Min Shi*, Muhammad Osama Khan*, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, 
Yu Tian, Luo Song, <strong>Ava Kouhana</strong>, Tobias Elze, Yi Fang, Mengyu Wang</p>
            <p>CVPR, 2024</p>
            <div class="paper-links">
              <a href="https://arxiv.org/pdf/2403.19949" class="paper-link" target="_blank">PDF</a>
              <a href="https://github.com/Harvard-Ophthalmology-AI-Lab/FairCLIP" class="paper-link" target="_blank">Code</a>
              <a href="https://drive.google.com/drive/u/1/folders/1bkeifigwOAfnsLvup9mJOSNeA3WsvA2l" class="paper-link" 
target="_blank">Dataset</a>
            </div>
          </div>
        </div>
    <div class="proceedings">
      <h2>Conference Proceedings</h2>
      <div class="paper">
        <div class="paper-content">
          <img src="images/pet-attenuation-scatter-correction-diffusion.jpg" alt="PET Attenuation and 
Scatter Correction using Diffusion Model" class="paper-image">
          <div class="paper-info">
            <h3 class="paper-title">Direct Generation of Attenuation and Scatter Correction of Brain PET 
Data Using a Conditional Latent Diffusion Model</h3>
            <p class="co-authors"><strong>Ava Kouhana</strong>, M. Jafaritadi, G. Chinn, C.S. Levin</p>
            <p>2024 IEEE Nuclear Science Symposium, Medical Imaging Conference (NSS MIC)</p>
            <div class="paper-links">
              <a href="#" class="paper-link" target="_blank">PDF</a>
              <a href="#" class="paper-link" target="_blank">Poster</a>
            </div>
          </div>
        </div>
      </div>
      <div class="paper">
        <div class="paper-content">
          <img src="images/super-resolution-tomographic-reconstruction.jpg" alt="Super-Resolution 
Tomographic Image Reconstruction" class="paper-image">
          <div class="paper-info">
            <h3 class="paper-title">Super-Resolution Tomographic Image Reconstruction Using Latent Diffusion 
Models</h3>
            <p class="co-authors"><strong>Ava Kouhana</strong>, M. Jafaritadi, G. Chinn, C.S. Levin</p>
            <p>2024 IEEE Nuclear Science Symposium, Medical Imaging Conference (NSS MIC)</p>
            <div class="paper-links">
              <a href="#" class="paper-link" target="_blank">PDF</a>
              <a href="#" class="paper-link" target="_blank">Poster</a>
            </div>
          </div>
        </div>
      </div>
    </div>
      <div class="paper">
        <div class="paper-content">
          <img src="images/fair-identity-scaling-dr-screening.jpg" alt="Fair Identity Scaling for Diabetic
Retinopathy Screening" class="paper-image">
          <div class="paper-info">
            <h3 class="paper-title">A New Deep Learning Technique Termed Fair Identity Scaling to Improve
Model Equity for Diabetic Retinopathy Screening</h3>
            <p class="co-authors"><strong>Ava Kouhana,</strong> Yan Luo, Yu Tian; Min Shi, Leo A. Kim,
Pasquale, Louis; Meenakashi Gupta, Tobias Elze, Lucia Sobrin, Mengyu Wang</p>
            <p>ARVO 2024 (Poster)</p>
<div class="paper-links">
  <a href="#" class="paper-link" target="_blank">PDF</a>
  <a href="posters/ARVO2024.pdf" class="paper-link" target="_blank">Poster</a>
</div>
          </div>
        </div>
      </div>
 <div class="awards">
      <h2>Grant</h2>
      <ul>
        <li><strong>Recipient of 2024 IEEE Nuclear Science, Medical Imaging Trainee Grant (NSS MIC)</strong></li>
      </ul>
    </div>
  </div>


</body>
</html>
