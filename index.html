<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" type="text/css" href="static/style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/light.css">
  <meta charset="UTF-8">
</head>
<body>

  <!-- Intro -->
  <div id="container">

    <!-- <div id="image-section">
      <img src="your-photo.jpg" alt="Your photo">
    </div> -->

    <div id="info-section">
      <h2><strong>Ava Kouhana </strong></h2>
      <p>
        <strong>akouhana (at) stanford.edu</strong> 
      </p>
      <p>
      I am a French master's student specializing in Artificial Intelligence and Statistics. Before commencing my research at Stanford University, I dedicated six months to 
conducting research at Harvard University's Ophthalmology Department. During this time, my focus was primarily on Computer Vision tasks (Image Segmentation, Vision-Language 
Models), as well as Diffusion models.

 </p>
 I am pleased to have joined Stanford in February 2024 for a six-month research opportunity in the Radiology Department under the 
supervision of Dr. Craig Levin where I worked primarily on Super-Resolution leveraging Latent Diffusion Models. From September 2024, I am fortunate to joining Stanford for the 
ICME ( Computational and Mathematical Engineering ) Master's degree.

      </p>
      <p>
      </p>
      <p>
        <a href="https://profiles.stanford.edu/intranet/ava-kouhana?tab=bio">[Profile]</a>
       <a href="https://scholar.google.com/citations?user=nxESuB0AAAAJ&hl=fr">[Scholar}</a>
	 <a href="https://www.linkedin.com/in/yonatano/">[LinkedIn]</a>

      </p>
    </div>
  </div>

  <!-- Papers -->
  <div id="container">
    <div id="papers-section">
      <h3>Papers</h3>
      <ul>
        <li>
          <h4 class="paper-title"><a href="https://openreview.net/pdf?id=qNrJJZAKI3">FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning 
	Using Segment Anything Model with Fair Error-Bound Scaling</a></h4>
          <p class="co-authors">Yu Tian*, Min Shi*,Yan Luo*,<strong> Ava Kouhana</strong>,Tobias Elze,Mengyu Wang. ICLR 2024.</p>
        </li>
        <li>
          <h4 class="paper-title"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_FairCLIP_Harnessing_Fairness_in_Vision-Language_Learning_CVPR_2024_paper.pdf">FairCLIP: Harnessing 
	Fairness in Vision-Language Learning</a></h4>
          <p class="co-authors">Yan Luo* ,Min Shi*, Muhammad Osama Khan* ,Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan,Yu Tian,Luo Song,<strong>Ava Kouhana</strong>, Tobias Elze, Yi Fang,Mengyu 
	Wang. CVPR 2024.</p>
        </li>
      </ul>
    </div>
  </div>

  <!-- Projects -->
  <!-- <div id="container">
    <div id="projects-section">
      <h3>Software / Projects</h3>
      <ul>
      </ul>
    </div>
  </div> -->

  <!-- Writing -->
  <!-- <div id="container">
    <div id="writing-section">
      <h3>Writing</h3>
      <ul>
      </ul>
    </div>
  </div> -->

  <!-- Misc -->
  <!-- <div id="container">
    <div id="misc-section" style="margin-bottom: 25px">
      <h3>Misc</h3>
    </div>
  </div> -->

  </div>

</body>
</html>
